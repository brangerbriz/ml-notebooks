{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "In this notebook, we take a look at using hyperparameter search to automate the process of selecting/fine tuning the hyperparameters used in our neural network. The parameters of a neural network are the weights and biases that are learned and continually updated during training. Networks often have thousands or even millions of parameters, and they are never adjusted by hand, but rather through some sort of automated process like backpropagation. In contrast, hyperparameters effect the architecture of the network itself (they can sort of be thought of as meta-parameters), and are often decided by humans. Things like number of layers, number of neurons per layer, and the learning rate or momentum of the network are all hyperparameters. \n",
    "\n",
    "Training neural networks can be very difficult and it is often the role of the programmer to select the hyperparameters that will yield the best results. It is not uncommon for a programmer to run 10 to 20 (or more) experiments, each with different hyperparameters, before arriving at an optimal solution (or even one that is simply \"good enough\").\n",
    "\n",
    "Hyperparameter search (often called Hyperparameter Optimization) is a method used to automate the discovery of effective hyperparameters for a network. Rather than using intuition and experience to fine-tune your hyperparameters by hand hyperparameter search can be used to automatically discover optimal hyperparameters given enough compute time and resources.\n",
    "\n",
    "In this example we are going to use hyperparameter search to discover a good network structure and appropriate learning rate, etc... for a classification task using the popular [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). We will be using the Keras library to construct our network, and [hyperas](https://github.com/maxpumperla/hyperas), a keras port of the hyperopt library, to automate our hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperas syntax is similar to that of a templating engine. It uses double curly-braces (`{{`) to denote hyperparameter content that should be replaced by hyperas at runtime. More on that soon, but for now it is important to realized that when you use hyperas, your code is pre-processed and many iterations of your source code are generated and run with the contents of each `{{...}}` replaced with a different set of parameters each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data() and Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the hyperas optmizer (`keras.optim`) to generate and evaluate different permutations of your source code it the must be given two special functions: `data()` and `model(X_train, Y_train, X_test, Y_test)`. \n",
    "\n",
    "`data()` is used to load and preprocess your data and must return `X_train, Y_train, X_test, Y_test`. This function is run only once by `keras.optim` to avoid reloading your data. To the best of my knowledge, templated `{{` statements are not permitted (or are at least not useful) inside of `data()`.\n",
    "\n",
    "`model(X_train, Y_train, X_test, Y_test)` is run once per an evaluation of a new hyperparameter configuration. This function is responsible for generating a model, training that model (in our case using `keras.fit(...)`), and returning a special dictionary object used by hyperas to evaluate the success of the hyperparameters chosen in that particular configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: For whatever reason, I've experienced a bug with hyperas that\n",
    "# prevents me from using any kind of comment in either the data() or\n",
    "# model() function. For this reason I will attempt to describe the \n",
    "# code in both of these functions through comments and explanations\n",
    "# outside of the functions themselves.\n",
    "def data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(60000, 784)\n",
    "    X_test = X_test.reshape(10000, 784)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    nb_classes = 10\n",
    "    Y_train = to_categorical(y_train, nb_classes)\n",
    "    Y_test = to_categorical(y_test, nb_classes)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if conditional({{choice(['three', 'four']) == 'four'}}):\n",
    "        model.add(Dense({{choice([64, 128, 256])}}))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "    \n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              nb_epoch=1,\n",
    "              show_accuracy=True,\n",
    "              verbose=2,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    \n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used three of the most common `hyperas` distribution functions: `choice`, `uniform`, and `conditional`. `choice([a, b, c, ...])` takes a list of possible values from which to select during hyperparameter search. `uniform(min, max)` samples from a uniform distribution between floats `min` and `max`. `conditional()` returns `True` or `False` based on the boolean contents inside of `{{}}` and can be used with `choice()` to easily include or exclude entire code blocks. For a complete list of hyperas (really hyperopt) distrobutions see [here](https://github.com/maxpumperla/hyperas/blob/master/hyperas/distributions.py).\n",
    "\n",
    "Note that `choice()` and `uniform()` are inside of the `{{}}` but with `conditional()` these brackets are inside the function parenthesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created our `data()` and `model(...)` functions, we are ready to execute our hyperparameter search using the `keras.optim.minimize(...)` function. This function performs `max_evals` versions of your `model(...)` function using different combinations of hyperparameter values attempting to minimize the \"loss\" value in the dictionary returned by each internal call to `model(...)`. It returns a tuple of the `(score, accuracy)` of the best run as well as the Keras model that achieved that run.\n",
    "\n",
    "__WARNING__: Performing a hyperparameter search in this way can take a __very long time__. The search space grows exponentially with each addition of a `hyperas` distribution function. The `max_evals` named parameter to `keras.optim.minimize(...)` can be used to limit the maximum number of hyperparameter searches that will be run. Also note that keeping the number of epochs low for each model fitting (i.e. `model.fit(X, y, nb_epoch=1)`) can drastically reduce the time it takes to execute the hyperparameter searches. The danger with this strategy is that hyperparameters, like learning rate, that take longer to achieve good results will be severly disadvantaged by the search and an optimal solution may be overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dense_1': hp.choice('Dense_1', [256, 512, 1024]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']) == 'four',\n",
      "        'Dense_2': hp.choice('Dense_2', [64, 128, 256]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
      "  3: X_train = X_train.reshape(60000, 784)\n",
      "  4: X_test = X_test.reshape(10000, 784)\n",
      "  5: X_train = X_train.astype('float32')\n",
      "  6: X_test = X_test.astype('float32')\n",
      "  7: X_train /= 255\n",
      "  8: X_test /= 255\n",
      "  9: nb_classes = 10\n",
      " 10: Y_train = to_categorical(y_train, nb_classes)\n",
      " 11: Y_test = to_categorical(y_test, nb_classes)\n",
      " 12: \n",
      " 13: \n",
      " 14: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     model = Sequential()\n",
      "  4:     model.add(Dense(512, input_dim=784))\n",
      "  5:     model.add(Activation('relu'))\n",
      "  6:     model.add(Dense(space['Dense']))\n",
      "  7:     model.add(Dropout(space['Dropout']))\n",
      "  8:     model.add(Activation(space['Activation']))\n",
      "  9:     model.add(Dense(space['Dense_1']))\n",
      " 10:     model.add(Dropout(space['Dropout_1']))\n",
      " 11:     \n",
      " 12:     if conditional(space['conditional']):\n",
      " 13:         model.add(Dense(space['Dense_2']))\n",
      " 14:         model.add(Dropout(0.5))\n",
      " 15:         \n",
      " 16:     model.add(Dense(10))\n",
      " 17:     model.add(Activation('softmax'))\n",
      " 18:     \n",
      " 19:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
      " 20:                   optimizer=space['optimizer'])\n",
      " 21:     \n",
      " 22:     model.fit(X_train, Y_train,\n",
      " 23:               batch_size=space['batch_size'],\n",
      " 24:               nb_epoch=1,\n",
      " 25:               show_accuracy=True,\n",
      " 26:               verbose=2,\n",
      " 27:               validation_data=(X_test, Y_test))\n",
      " 28:     \n",
      " 29:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      " 30:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 31: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:635: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "3s - loss: 0.3570 - acc: 0.8931 - val_loss: 0.1370 - val_acc: 0.9614\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3131 - acc: 0.9032 - val_loss: 0.1155 - val_acc: 0.9637\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "3s - loss: 0.8328 - acc: 0.7466 - val_loss: 0.3436 - val_acc: 0.9056\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "4s - loss: 0.3271 - acc: 0.9098 - val_loss: 0.2653 - val_acc: 0.9438\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "2s - loss: 1.0171 - acc: 0.7238 - val_loss: 0.4397 - val_acc: 0.8874\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "1s - loss: 1.9475 - acc: 0.4085 - val_loss: 1.3285 - val_acc: 0.7453\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "2s - loss: 0.3250 - acc: 0.8994 - val_loss: 0.1334 - val_acc: 0.9583\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "3s - loss: 1.6659 - acc: 0.4631 - val_loss: 0.7455 - val_acc: 0.8229\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "1s - loss: 1.3981 - acc: 0.5454 - val_loss: 0.5619 - val_acc: 0.8662\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "2s - loss: 0.9324 - acc: 0.6984 - val_loss: 0.2420 - val_acc: 0.9311\n",
      "Evalutation of best performing model:\n",
      " 9856/10000 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[0.11549773814436048, 0.9637]\n",
      "{'batch_size': 1, 'Dense_1': 1, 'Dense': 1, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.5141896051963405, 'Dropout': 0.6676204909178812, 'optimizer': 1} <keras.models.Sequential object at 0x7f7b10d335f8>\n",
      "[{'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [0], 'Dense_1': [0], 'Dense': [0], 'Activation': [0], 'Dense_2': [0], 'Dropout_1': [0], 'Dropout': [0], 'optimizer': [0]}, 'tid': 0, 'workdir': None, 'vals': {'batch_size': 0, 'Dense_1': 2, 'Dense': 0, 'Activation': 1, 'Dense_2': 2, 'Dropout_1': 0.28589324501392466, 'Dropout': 0.3013289168318932, 'optimizer': 0}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 42, 55, 227000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 42, 59, 202000), 'owner': None, 'tid': 0, 'result': {'loss': -0.9614, 'model': <keras.models.Sequential object at 0x7f7b10bb08d0>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [1], 'Dense_1': [1], 'Dense': [1], 'Activation': [1], 'Dense_2': [1], 'Dropout_1': [1], 'Dropout': [1], 'optimizer': [1]}, 'tid': 1, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 1, 'Dense': 1, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.5141896051963405, 'Dropout': 0.6676204909178812, 'optimizer': 1}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 42, 59, 205000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 2, 560000), 'owner': None, 'tid': 1, 'result': {'loss': -0.9637, 'model': <keras.models.Sequential object at 0x7f7b10d335f8>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [2], 'Dense_1': [2], 'Dense': [2], 'Activation': [2], 'Dense_2': [2], 'Dropout_1': [2], 'Dropout': [2], 'optimizer': [2]}, 'tid': 2, 'workdir': None, 'vals': {'batch_size': 0, 'Dense_1': 0, 'Dense': 2, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.05380209675405134, 'Dropout': 0.7544493988763304, 'optimizer': 2}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 2, 564000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 6, 406000), 'owner': None, 'tid': 2, 'result': {'loss': -0.9056, 'model': <keras.models.Sequential object at 0x7f7b07cbdda0>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [3], 'Dense_1': [3], 'Dense': [3], 'Activation': [3], 'Dense_2': [3], 'Dropout_1': [3], 'Dropout': [3], 'optimizer': [3]}, 'tid': 3, 'workdir': None, 'vals': {'batch_size': 0, 'Dense_1': 2, 'Dense': 2, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.953001855978533, 'Dropout': 0.1525985853561005, 'optimizer': 0}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 6, 409000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 11, 370000), 'owner': None, 'tid': 3, 'result': {'loss': -0.9438, 'model': <keras.models.Sequential object at 0x7f7b07838470>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [4], 'Dense_1': [4], 'Dense': [4], 'Activation': [4], 'Dense_2': [4], 'Dropout_1': [4], 'Dropout': [4], 'optimizer': [4]}, 'tid': 4, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 2, 'Dense': 2, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.3910858065581043, 'Dropout': 0.31446135612718296, 'optimizer': 2}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 11, 372000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 14, 298000), 'owner': None, 'tid': 4, 'result': {'loss': -0.8874, 'model': <keras.models.Sequential object at 0x7f7b0723eda0>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [5], 'Dense_1': [5], 'Dense': [5], 'Activation': [5], 'Dense_2': [5], 'Dropout_1': [5], 'Dropout': [5], 'optimizer': [5]}, 'tid': 5, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 0, 'Dense': 0, 'Activation': 1, 'Dense_2': 1, 'Dropout_1': 0.4468801063046538, 'Dropout': 0.1507315782105022, 'optimizer': 2}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 14, 301000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 16, 760000), 'owner': None, 'tid': 5, 'result': {'loss': -0.7453, 'model': <keras.models.Sequential object at 0x7f7b06d27c18>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [6], 'Dense_1': [6], 'Dense': [6], 'Activation': [6], 'Dense_2': [6], 'Dropout_1': [6], 'Dropout': [6], 'optimizer': [6]}, 'tid': 6, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 0, 'Dense': 1, 'Activation': 1, 'Dense_2': 2, 'Dropout_1': 0.31105102812539254, 'Dropout': 0.28629243690425654, 'optimizer': 1}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 16, 763000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 19, 784000), 'owner': None, 'tid': 6, 'result': {'loss': -0.9583, 'model': <keras.models.Sequential object at 0x7f7b068dfef0>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [7], 'Dense_1': [7], 'Dense': [7], 'Activation': [7], 'Dense_2': [7], 'Dropout_1': [7], 'Dropout': [7], 'optimizer': [7]}, 'tid': 7, 'workdir': None, 'vals': {'batch_size': 0, 'Dense_1': 2, 'Dense': 1, 'Activation': 1, 'Dense_2': 1, 'Dropout_1': 0.87854671845424, 'Dropout': 0.25150491930845165, 'optimizer': 2}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 19, 787000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 23, 575000), 'owner': None, 'tid': 7, 'result': {'loss': -0.8229, 'model': <keras.models.Sequential object at 0x7f7b06302978>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [8], 'Dense_1': [8], 'Dense': [8], 'Activation': [8], 'Dense_2': [8], 'Dropout_1': [8], 'Dropout': [8], 'optimizer': [8]}, 'tid': 8, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 2, 'Dense': 0, 'Activation': 0, 'Dense_2': 0, 'Dropout_1': 0.5852134195333071, 'Dropout': 0.6296197127160789, 'optimizer': 2}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 23, 578000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 26, 217000), 'owner': None, 'tid': 8, 'result': {'loss': -0.8662, 'model': <keras.models.Sequential object at 0x7f7b05d0a860>, 'status': 'ok'}, 'spec': None}, {'state': 2, 'exp_key': None, 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'), 'idxs': {'batch_size': [9], 'Dense_1': [9], 'Dense': [9], 'Activation': [9], 'Dense_2': [9], 'Dropout_1': [9], 'Dropout': [9], 'optimizer': [9]}, 'tid': 9, 'workdir': None, 'vals': {'batch_size': 1, 'Dense_1': 2, 'Dense': 2, 'Activation': 0, 'Dense_2': 1, 'Dropout_1': 0.7085536816213929, 'Dropout': 0.9704582190325389, 'optimizer': 1}}, 'book_time': datetime.datetime(2017, 1, 16, 22, 43, 26, 220000), 'version': 0, 'refresh_time': datetime.datetime(2017, 1, 16, 22, 43, 29, 839000), 'owner': None, 'tid': 9, 'result': {'loss': -0.9311, 'model': <keras.models.Sequential object at 0x7f7b058989e8>, 'status': 'ok'}, 'spec': None}]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      notebook_name='hyperparameter_search',\n",
    "                                      trials=trials)\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(best_run, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
