{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "In this notebook, we take a look at using hyperparameter search to automate the process of selecting/fine tuning the hyperparameters used in our neural network. The parameters of a neural network are the weights and biases that are learned and continually updated during training. Networks often have thousands or even millions of parameters, and they are never adjusted by hand, but rather through some sort of automated process like backpropagation. In contrast, hyperparameters effect the architecture of the network itself (they can sort of be thought of as meta-parameters), and are often decided by humans. Things like number of layers, number of neurons per layer, and the learning rate or momentum of the network are all hyperparameters. \n",
    "\n",
    "Training neural networks can be very difficult and it is often the role of the programmer to select the hyperparameters that will yield the best results. It is not uncommon for a programmer to run 10 to 20 (or more) experiments, each with different hyperparameters, before arriving at an optimal solution (or even one that is simply \"good enough\").\n",
    "\n",
    "Hyperparameter search (often called Hyperparameter Optimization) is a method used to automate the discovery of effective hyperparameters for a network. Rather than using intuition and experience to fine-tune your hyperparameters by hand, hyperparameter search can be used to automatically discover optimal hyperparameters given enough compute time and resources.\n",
    "\n",
    "In this example we are going to use hyperparameter search to discover a good network architecture and appropriate learning rate, etc... for a classification task using the popular [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). We will be using the Keras library to construct our network, and [hyperas](https://github.com/maxpumperla/hyperas), a keras port of the [hyperopt](http://hyperopt.github.io/hyperopt/) library, to automate our hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dill as pickle\n",
    "import pprint as pprint\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperas syntax is similar to that of a templating engine. It uses double curly-braces (`{{`) to denote hyperparameter content that should be replaced by hyperas at runtime. More on that soon, but for now it is important to realized that when you use hyperas, your code is pre-processed and many iterations of your source code are generated and run with the contents of each `{{...}}` replaced with a different set of hyperparameters each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data() and Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the hyperas optmizer (`keras.optim`) to generate and evaluate different permutations of your source code it the must be given two special functions: `data()` and `model(X_train, Y_train, X_test, Y_test)`. \n",
    "\n",
    "`data()` is used to load and preprocess your data and must return `X_train, y_train, X_test, y_test`. This function is run only once by `keras.optim` to avoid reloading your data. To the best of my knowledge, templated `{{` statements are not permitted (or are at least not useful) inside of `data()`.\n",
    "\n",
    "`model(X_train, Y_train, X_test, Y_test)` is run once per an evaluation of a new hyperparameter combination. This function is responsible for generating a model, training that model (in our case using `keras.fit(...)`), and returning a special dictionary object used by hyperas to evaluate the success of the hyperparameters chosen in that particular configuration (more on this dictionary below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: For whatever reason, I've experienced a bug with hyperas that\n",
    "# prevents me from using any kind of comment in either the data() or\n",
    "# model() function. For this reason I will attempt to describe the \n",
    "# code in both of these functions through comments and explanations\n",
    "# outside of the functions themselves.\n",
    "def data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(60000, 784)\n",
    "    X_test = X_test.reshape(10000, 784)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    nb_classes = 10\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Dropout({{uniform(0.25, 0.75)}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Dropout({{uniform(0.25, 0.75)}}))\n",
    "    \n",
    "    if conditional({{choice(['three', 'four']) == 'four'}}):\n",
    "        model.add(Dense({{choice([64, 128, 256])}}))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              nb_epoch=1,\n",
    "              show_accuracy=True,\n",
    "              verbose=0,\n",
    "              validation_data=(X_test, y_test))\n",
    "    \n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used three of the most common `hyperas` distribution functions: `choice`, `uniform`, and `conditional`. `choice([a, b, c, ...])` takes a list of possible values from which to select during hyperparameter search. `uniform(min, max)` samples from a uniform distribution between floats `min` and `max`. `conditional()` returns `True` or `False` based on the boolean contents inside of `{{}}` and can be used with `choice()` to easily include or exclude entire code blocks. For a complete list of hyperas (really hyperopt) distrobutions see [here](https://github.com/maxpumperla/hyperas/blob/master/hyperas/distributions.py).\n",
    "\n",
    "The dictionary returned by `model(...)` must include two required key => value pairs: `loss`, and `status`. `loss` is the value you are trying to minimize through your hyperparameter search. Think of this as the value you are looking to produce given optimal hyperparameters. In our case we are looking for the hyperparameters that give us the highest accuracy in an MNIST classification problem. Because hyperas requires the `loss` value to be minimized during search, and we are looking to maximize accuracy, we invert the sign of our accuracy when assigning it to the value of the loss key in the returned dictionary. Thus we convert `acc` to `-acc`. \n",
    "\n",
    "The value of `status` is simply a constant representing whether or not the model should be considered as successful and included in our record of trials. Unless there is an error that is caught or some anomoly in your `model(...)` function this value can be `STATUS_OK`. In addition to these two required keys, you are free to add as many additional key => value pairs to the returned dictionary as you like, essentially making variables local to the `model(...)` function available to the `Trials()` object used in `keras.optim`. A common extra key to use is `model` that allows you to return a reference to the trained model itself.\n",
    "\n",
    "__Note__: `choice()` and `uniform()` are inside of the `{{}}` but with `conditional()` these brackets are inside the function parenthesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created our `data()` and `model(...)` functions, we are ready to execute our hyperparameter search using the `keras.optim.minimize(...)` function. This function performs `max_evals` versions of your `model(...)` function using different combinations of hyperparameter values attempting to minimize the \"loss\" value in the dictionary returned by each internal call to `model(...)`. It returns a tuple of the `(score, accuracy)` of the best run as well as the Keras model that achieved that run.\n",
    "\n",
    "__WARNING__: Performing a hyperparameter search in this way can take a __very long time__. The search space grows exponentially with each addition of a `hyperas` distribution function. The `max_evals` named parameter to `keras.optim.minimize(...)` can be used to limit the maximum number of hyperparameter searches that will be run. Also note that keeping the number of epochs low for each model fitting (i.e. `model.fit(X, y, nb_epoch=1)`) can drastically reduce the time it takes to execute the hyperparameter searches. The danger with this strategy is that hyperparameters, like learning rate, that take longer to achieve good results will be severly disadvantaged by the search and an optimal solution may be overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:635: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5s - loss: 0.2475 - acc: 0.9324 - val_loss: 0.2196 - val_acc: 0.9528\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "6s - loss: 0.2891 - acc: 0.9140 - val_loss: 0.1190 - val_acc: 0.9659\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "4s - loss: 0.2600 - acc: 0.9272 - val_loss: 0.1242 - val_acc: 0.9687\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "5s - loss: 0.2253 - acc: 0.9324 - val_loss: 0.1093 - val_acc: 0.9655\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "6s - loss: 0.2341 - acc: 0.9303 - val_loss: 0.1305 - val_acc: 0.9617\n",
      "Best run scored was\n",
      "{'Dense_1': 2, 'Activation': 0, 'Dense_2': 0, 'Dense': 0, 'optimizer': 0, 'Dropout_1': 0.2676392100975012, 'Dropout': 0.3428208352348263}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_run, foo = optim.minimize(model=model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=1000,\n",
    "                          notebook_name='hyperparameter_search',\n",
    "                          verbose=False,\n",
    "                          trials=trials)\n",
    "\n",
    "print(\"Best run scored was\")\n",
    "print(best_run)\n",
    "\n",
    "# serialize and save the trials object\n",
    "try:\n",
    "    pickle.dump(trials.trials, open('../data/hyperparameter_trials.pickel', 'wb'), recurse=True, byref=False)\n",
    "except e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('../data/hyperparameter_trials.pickel', 'rb')\n",
    "t = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
