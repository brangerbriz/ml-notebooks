{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing a Dataset\n",
    "\n",
    "A vital first step in many machine learning tasks is to preprocess your dataset. In this notebook we will run an experiment using two types of dataset preprocessing: regularization (also called normalization) and feature standardization (also called scaling). We will then train a simple neural network with data processed using these two methods of preprocessing and compare them to a baseline of the non-preprocessed dataset.\n",
    "\n",
    "We will train three different models using data transformed in these ways:\n",
    "- No preprocessing (baseline)\n",
    "- Regularization\n",
    "- Feature Standardization\n",
    "\n",
    "Regularization (normalization) transforms all of the values present in the dataset so that they lie between 0.0 and 1.0. Sometimes it is helpful to transform these values so they are are between -1.0 and 1.0 instead of 0.0 and 1.0.\n",
    "\n",
    "Feature standardization transforms the data so that it has a Gaussian distrobution with zero mean (zero-centered) and unit variance. For each feature, this is achieved by subtracting the mean of that feature across the dataset and then dividing by that feature's standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports, etc...\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train/Test\n",
    "For this task we are attempting to classify Pokemon (grass, water, etc...) given 6 statistics like HP, Attack, Speed, etc... as features. I recognize this isn't the most exciting classification problem but it was a weekly coding challenge from Siraj Raval's [dataset preperation video](https://www.youtube.com/watch?v=0xVqLJe9_CY). The 5-7 participants that submitted results seemed to have a classification accuracy from ~14% to 75% with a mode of ~30%.\n",
    "\n",
    "Our first step is to parse the `Pokemon.csv`, extract only the features that we want, and then split our data into testing/training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 training samples\n",
      "200 testing samples\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337) # l337 4 lyf3\n",
    "\n",
    "features, labels = ([], [])\n",
    "with open('../data/Pokemon.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        features.append([*row[5:11]]) \n",
    "        labels.append(row[2])\n",
    "\n",
    "# remove column names\n",
    "features.pop(0)\n",
    "labels.pop(0)\n",
    "\n",
    "# convert labels from strings to ints\n",
    "uniq_labels = list(set(labels))\n",
    "label_2_int = dict((key, value) for key, value in zip(uniq_labels, range(len(uniq_labels))))\n",
    "labels = to_categorical(list(map(lambda l: label_2_int[l], labels)))\n",
    "\n",
    "# shuffle the data\n",
    "features, labels = shuffle(np.array(features).astype(np.float),\n",
    "                           np.array(labels).astype(np.int), random_state=0)\n",
    "\n",
    "#split 75% training, 25% testing\n",
    "split = math.floor(len(labels) * 0.75) \n",
    "train_X, train_y = (features[0:split], labels[0:split])\n",
    "test_X, test_y = (features[split:], labels[split:])\n",
    "\n",
    "print('{} training samples'.format(len(train_X)))\n",
    "print('{} testing samples'.format(len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# baseline, no preprocessing at all, not even normalization\n",
    "train_X_base = np.copy(train_X)\n",
    "test_X_base = np.copy(test_X)\n",
    "\n",
    "# regularization, scale values so that they are between 0.0 and 1.0\n",
    "train_X_norm = np.nan_to_num((train_X - np.min(train_X)) / (np.max(train_X) - np.min(train_X)))\n",
    "test_X_norm = np.nan_to_num((test_X - np.min(train_X)) / (np.max(train_X) - np.min(train_X)))\n",
    "\n",
    "# feature standardization\n",
    "# Note: This can also be achieved with sklearn preprocessing.scale(...) or\n",
    "# preprocessing.StandardScaler(...) but I choose to do it with vanilla numpy\n",
    "# here for demonstration.\n",
    "train_X_std = np.copy(train_X)\n",
    "test_X_std = np.copy(test_X)\n",
    "# for each feature, subtract the mean and divide by the std dev\n",
    "train_X_std = np.nan_to_num((train_X_std - np.mean(train_X, axis=0)) / np.std(train_X, axis=0))\n",
    "# note: we use the mean and std dev of the training set (train_X) because the\n",
    "# test set is theoretically yet \"unseen\" by the algorithm\n",
    "test_X_std = np.nan_to_num((test_X_std - np.mean(train_X, axis=0)) / np.std(train_X, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifiers\n",
    "This can take a while depending on the resources you have access to. If you see an asterisk in the `In [*]` to the left the below cell is still processing. Upon completion you should see the three printed statements below. If it is taking entirely too long try with less neurons per layer: `Dense(XXX, ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classifier training complete.\n",
      "Regularized classifier training complete.\n"
     ]
    }
   ],
   "source": [
    "def get_model(num_inputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=num_inputs, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(18, init='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf_base = get_model(train_X.shape[1])\n",
    "clf_norm = get_model(train_X.shape[1])\n",
    "clf_std = get_model(train_X.shape[1])\n",
    "\n",
    "clf_base.fit(train_X_base, train_y, nb_epoch=1000, verbose=0)\n",
    "print('Base classifier training complete.')\n",
    "clf_norm.fit(train_X_norm, train_y, nb_epoch=1000, verbose=0)\n",
    "print('Regularized classifier training complete.')\n",
    "clf_std.fit(train_X_std, train_y, nb_epoch=1000, verbose=0)\n",
    "print('Standardized classifier training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Base: 0.30\n",
      "Accuracy of Normalized: 0.28\n",
      "Accuracy of Standardized: 0.24\n"
     ]
    }
   ],
   "source": [
    "def print_accuracy(name, expected, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if np.argmax(predicted[i]) == np.argmax(expected[i]):\n",
    "            correct += 1\n",
    "    print('Accuracy of %s: %.2f' % (name, correct / len(test_y)))\n",
    "\n",
    "pred_base = clf_base.predict(test_X_base)\n",
    "pred_norm = clf_norm.predict(test_X_norm)\n",
    "pred_std = clf_std.predict(test_X_std)\n",
    "\n",
    "# print(np.sum(pred_norm, axis=1))\n",
    "\n",
    "print_accuracy('Base', test_y, pred_base)\n",
    "print_accuracy('Regularized', test_y, pred_norm)\n",
    "print_accuracy('Standardized', test_y, pred_std)\n",
    "\n",
    "y = [np.argmax(pred) for pred in test_y]\n",
    "base = [np.argmax(pred) for pred in pred_base]\n",
    "norm = [np.argmax(pred) for pred in pred_norm]\n",
    "std = [np.argmax(pred) for pred in pred_std]\n",
    "\n",
    "# uncomment for more verbose logging...\n",
    "message = \"\\nClassification report for %s:\\n%s\\n\"\n",
    "# print(message % ('base', metrics.classification_report(y, base)))\n",
    "# print(message % ('norm', metrics.classification_report(y, norm)))\n",
    "# print(message % ('std', metrics.classification_report(y, std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the regularized and standardized datasets performed similarly (or maybe even worse) than the baseline. I've found that this is sometimes the case, and just because data is preprocessed doesn't meen that it will necessarily perform better than the original data. That said, preprocessing is a very important step in the ML pipeline and you should always use it to at least see if you can get a better result than the original data. Various datasets and algorithms will require it more than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
